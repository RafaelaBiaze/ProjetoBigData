{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe57fcb-248b-407e-a4c3-86bda548b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install pyspark\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, lit, lower, regexp_replace\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntegerType, DoubleType\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# --- 1. Inicialização do Spark ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessamentoSilver\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Sessão Spark iniciada para Processamento SILVER.\")\n",
    "\n",
    "# --- 2. Variáveis e Schemas ---\n",
    "# Caminhos locais para ler os dados brutos criados no script anterior\n",
    "CAMINHO_BRONZE_SUICIDIO = \"dados_bronze_suicidio.json\"\n",
    "CAMINHO_BRONZE_DEPRESSAO = \"dados_bronze_depressao.json\"\n",
    "\n",
    "# Caminhos do HDFS para salvar a Camada Silver\n",
    "CAMINHO_SILVER_SUICIDIO = \"hdfs://namenode:9000/datalake/silver/suicidio\"\n",
    "CAMINHO_SILVER_DEPRESSAO = \"hdfs://namenode:9000/datalake/silver/depressao\"\n",
    "\n",
    "# Schema do dado bruto (manualmente definido para segurança)\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"SpatialDim\", StringType(), True),\n",
    "    StructField(\"TimeDim\", IntegerType(), True),\n",
    "    StructField(\"Dim1\", StringType(), True),\n",
    "    StructField(\"NumericValue\", DoubleType(), True),\n",
    "    # ... (outros campos do schema original, mantendo a estrutura)\n",
    "    StructField(\"IndicatorCode\", StringType(), True),\n",
    "    StructField(\"SpatialDimType\", StringType(), True),\n",
    "    StructField(\"ParentLocationCode\", StringType(), True),\n",
    "    StructField(\"TimeDimType\", StringType(), True),\n",
    "    StructField(\"ParentLocation\", StringType(), True),\n",
    "    StructField(\"Dim1Type\", StringType(), True),\n",
    "    StructField(\"Dim2Type\", StringType(), True),\n",
    "    StructField(\"Dim2\", StringType(), True),\n",
    "    StructField(\"Dim3Type\", StringType(), True),\n",
    "    StructField(\"Dim3\", StringType(), True),\n",
    "    StructField(\"DataSourceDimType\", StringType(), True),\n",
    "    StructField(\"DataSourceDim\", StringType(), True),\n",
    "    StructField(\"Value\", StringType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"TimeDimensionValue\", StringType(), True),\n",
    "    StructField(\"TimeDimensionBegin\", StringType(), True),\n",
    "    StructField(\"TimeDimensionEnd\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- 3. Função de Criação e Tratamento do DataFrame (Camada Silver) ---\n",
    "def criar_df_tratado(caminho_dados, dataset_nome):\n",
    "    print(f\"Lendo dados brutos de {caminho_dados}...\")\n",
    "    try:\n",
    "        with open(caminho_dados, 'r') as f:\n",
    "            dados = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo {caminho_dados} não encontrado. Execute ingestao_bronze.py primeiro.\")\n",
    "        return None\n",
    "\n",
    "    # Converte a lista de JSONs em RDD e depois em DataFrame\n",
    "    json_strings = [json.dumps(item) for item in dados]\n",
    "    rdd = spark.sparkContext.parallelize(json_strings)\n",
    "    df = spark.read.json(rdd, schema=schema)\n",
    "    \n",
    "    # Aplicando transformações (limpeza e organização)\n",
    "    df_tratado = df.select(\n",
    "        col(\"SpatialDim\").alias(\"Pais\"),\n",
    "        col(\"TimeDim\").alias(\"Ano\"),\n",
    "        col(\"Dim1\").alias(\"Sexo\"),\n",
    "        col(\"NumericValue\").alias(\"Valor\")\n",
    "    ).withColumn(\"dataset_origem\", lit(dataset_nome))\n",
    "    \n",
    "    return df_tratado\n",
    "\n",
    "# --- 4. Execução do Processamento e Carga na Camada SILVER ---\n",
    "df_suicidio = criar_df_tratado(CAMINHO_BRONZE_SUICIDIO, \"suicidio\")\n",
    "df_depressao = criar_df_tratado(CAMINHO_BRONZE_DEPRESSAO, \"depressao\")\n",
    "\n",
    "if df_suicidio is None or df_depressao is None:\n",
    "    spark.stop()\n",
    "    exit()\n",
    "\n",
    "print(\"\\nSalvando dados tratados (Camada SILVER) no HDFS...\")\n",
    "df_suicidio.write.mode(\"overwrite\").parquet(CAMINHO_SILVER_SUICIDIO)\n",
    "df_depressao.write.mode(\"overwrite\").parquet(CAMINHO_SILVER_DEPRESSAO)\n",
    "\n",
    "print(\"Dados SILVER salvos com sucesso no HDFS!\")\n",
    "df_suicidio.show(5)\n",
    "df_depressao.show(5)\n",
    "\n",
    "# --- 5. Finalização ---\n",
    "spark.stop()\n",
    "print(\"Sessão Spark de Processamento SILVER finalizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
