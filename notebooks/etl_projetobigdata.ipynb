{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e1501",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão Spark iniciada com sucesso.\n",
      "Dados coletados com sucesso! Total de registros retornados: 185\n",
      "Dados coletados com sucesso! Total de registros retornados: 185\n",
      "root\n",
      " |-- Pais: string (nullable = true)\n",
      " |-- Ano: integer (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Valor: double (nullable = true)\n",
      " |-- dataset_origem: string (nullable = false)\n",
      "\n",
      "+----+----+--------+-----------+--------------+\n",
      "|Pais|Ano |Sexo    |Valor      |dataset_origem|\n",
      "+----+----+--------+-----------+--------------+\n",
      "|GHA |2021|SEX_BTSX|5.316740355|suicidio      |\n",
      "|JPN |2021|SEX_BTSX|17.43068584|suicidio      |\n",
      "|TGO |2021|SEX_BTSX|9.340400499|suicidio      |\n",
      "|LSO |2021|SEX_BTSX|28.66132363|suicidio      |\n",
      "|AZE |2021|SEX_BTSX|1.558857505|suicidio      |\n",
      "+----+----+--------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Pais: string (nullable = true)\n",
      " |-- Ano: integer (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Valor: double (nullable = true)\n",
      " |-- dataset_origem: string (nullable = false)\n",
      "\n",
      "+----+----+--------+-----------+--------------+\n",
      "|Pais|Ano |Sexo    |Valor      |dataset_origem|\n",
      "+----+----+--------+-----------+--------------+\n",
      "|ZWE |2021|SEX_BTSX|25.42332703|depressao     |\n",
      "|USA |2021|SEX_BTSX|14.21562781|depressao     |\n",
      "|PNG |2021|SEX_BTSX|2.144471239|depressao     |\n",
      "|CIV |2021|SEX_BTSX|12.66586382|depressao     |\n",
      "|LBY |2021|SEX_BTSX|5.102921402|depressao     |\n",
      "+----+----+--------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Salvando dados tratados no HDFS...\n",
      "Dados salvos com sucesso no HDFS!\n",
      "DataFrames Silver carregados:\n",
      "+----+----+--------+-----------+--------------+\n",
      "|Pais| Ano|    Sexo|      Valor|dataset_origem|\n",
      "+----+----+--------+-----------+--------------+\n",
      "| GHA|2021|SEX_BTSX|5.316740355|      suicidio|\n",
      "| JPN|2021|SEX_BTSX|17.43068584|      suicidio|\n",
      "| TGO|2021|SEX_BTSX|9.340400499|      suicidio|\n",
      "| LSO|2021|SEX_BTSX|28.66132363|      suicidio|\n",
      "| AZE|2021|SEX_BTSX|1.558857505|      suicidio|\n",
      "+----+----+--------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+----+--------+-----------+--------------+\n",
      "|Pais| Ano|    Sexo|      Valor|dataset_origem|\n",
      "+----+----+--------+-----------+--------------+\n",
      "| ZWE|2021|SEX_BTSX|25.42332703|     depressao|\n",
      "| USA|2021|SEX_BTSX|14.21562781|     depressao|\n",
      "| PNG|2021|SEX_BTSX|2.144471239|     depressao|\n",
      "| CIV|2021|SEX_BTSX|12.66586382|     depressao|\n",
      "| LBY|2021|SEX_BTSX|5.102921402|     depressao|\n",
      "+----+----+--------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Resultado do join GOLD:\n",
      "+----+----+--------+-------------+--------------+\n",
      "|Pais|Ano |Sexo    |Taxa_Suicidio|Taxa_Depressao|\n",
      "+----+----+--------+-------------+--------------+\n",
      "|CIV |2021|SEX_BTSX|7.391310765  |12.66586382   |\n",
      "|ZWE |2021|SEX_BTSX|17.34344471  |25.42332703   |\n",
      "|RWA |2021|SEX_BTSX|8.677832781  |13.64981111   |\n",
      "|SEN |2021|SEX_BTSX|6.758277028  |11.56092209   |\n",
      "|PNG |2021|SEX_BTSX|1.817868107  |2.144471239   |\n",
      "|USA |2021|SEX_BTSX|15.63138897  |14.21562781   |\n",
      "|AZE |2021|SEX_BTSX|1.558857505  |1.559577648   |\n",
      "|IRN |2021|SEX_BTSX|4.07295737   |4.084718064   |\n",
      "|ZAF |2021|SEX_BTSX|22.29580067  |21.11206779   |\n",
      "|SVN |2021|SEX_BTSX|18.8503857   |13.19910584   |\n",
      "+----+----+--------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Arquivo GOLD salvo em Parquet com sucesso!\n",
      "Tabela salva com sucesso no PostgreSQL!\n",
      "Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\"\"\"\n",
    "Função principal para orquestrar o pipeline de dados.\n",
    "\"\"\"\n",
    "# --- 1. Inicialização do Spark ---\n",
    "# Cria uma sessão Spark, que é o ponto de entrada para qualquer funcionalidade do Spark.\n",
    "# O appName ajuda a identificar sua aplicação na UI do Spark.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_SuicidioXDepressao\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Sessão Spark iniciada com sucesso.\")\n",
    "\n",
    "# --- 2. Extração (Extract) ---\n",
    "# Coletamos os dados da API usando a biblioteca 'requests' do Python.\n",
    "# Esta etapa é executada no Driver do Spark.\n",
    "both = \"SEX_BTSX\"\n",
    "female = \"SEX_FMLE\"\n",
    "male = \"SEX_MLE\"\n",
    "\n",
    "URL_SUICIDIO_B = f\"https://ghoapi.azureedge.net/api/SDGSUICIDE?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{both}' and Dim2 eq 'AGEGROUP_YEARSALL'\"\n",
    "URL_SUICIDIO_F = f\"https://ghoapi.azureedge.net/api/SDGSUICIDE?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{female}' and Dim2 eq 'AGEGROUP_YEARSALL'\"\n",
    "URL_SUICIDIO_M = f\"https://ghoapi.azureedge.net/api/SDGSUICIDE?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{male}' and Dim2 eq 'AGEGROUP_YEARSALL'\"\n",
    "\n",
    "URL_DEPRESSAO_B = f\"https://ghoapi.azureedge.net/api/MH_12?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{both}'\"\n",
    "URL_DEPRESSAO_F = f\"https://ghoapi.azureedge.net/api/MH_12?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{female}'\"\n",
    "URL_DEPRESSAO_M = f\"https://ghoapi.azureedge.net/api/MH_12?$filter=TimeDim eq 2021 and SpatialDimType eq 'COUNTRY' and Dim1 eq '{male}'\"\n",
    "\n",
    "# É uma boa prática definir um cabeçalho (header) com uma chave de API.\n",
    "# Para este exemplo, a API funciona sem chave, mas em produção, você deve usar uma.\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def extrair_api(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Lança um erro para respostas com status 4xx/5xx\n",
    "        dados = response.json().get(\"value\",[])\n",
    "        print(f\"Dados coletados com sucesso! Total de registros retornados: {len(dados)}\")\n",
    "        return dados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao chamar a API: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Extraindo dados de cada api:\")\n",
    "dados_suicidio_b = extrair_api(URL_SUICIDIO_B)\n",
    "dados_suicidio_f = extrair_api(URL_SUICIDIO_F)\n",
    "dados_suicidio_m = extrair_api(URL_SUICIDIO_M)\n",
    "\n",
    "dados_depressao_b = extrair_api(URL_DEPRESSAO_B)\n",
    "dados_depressao_f = extrair_api(URL_DEPRESSAO_F)\n",
    "dados_depressao_m = extrair_api(URL_DEPRESSAO_M)\n",
    "\n",
    "# Se a resposta estiver vazia, não há o que processar.\n",
    "if not dados_suicidio_b and dados_suicidio_f and dados_suicidio_m and dados_depressao_b and dados_depressao_f and dados_depressao_m:\n",
    "    print(\"Nenhum dado retornado pela API. Encerrando o processo...\")\n",
    "    spark.stop()\n",
    "\n",
    "# --- 3. Transformação (Transform) ---\n",
    "# Agora, vamos converter os dados JSON em um DataFrame Spark e tratá-los.\n",
    "\n",
    "# O Spark pode inferir o schema, mas definir explicitamente é mais seguro e performático.\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"IndicatorCode\", StringType(), True),\n",
    "\n",
    "    StructField(\"SpatialDimType\", StringType(), True),\n",
    "    StructField(\"SpatialDim\", StringType(), True),\n",
    "\n",
    "    StructField(\"ParentLocationCode\", StringType(), True),\n",
    "    StructField(\"TimeDimType\", StringType(), True),\n",
    "    StructField(\"ParentLocation\", StringType(), True),\n",
    "\n",
    "    StructField(\"Dim1Type\", StringType(), True),\n",
    "    StructField(\"TimeDim\", IntegerType(), True),\n",
    "    StructField(\"Dim1\", StringType(), True),\n",
    "\n",
    "    StructField(\"Dim2Type\", StringType(), True),\n",
    "    StructField(\"Dim2\", StringType(), True),\n",
    "\n",
    "    StructField(\"Dim3Type\", StringType(), True),\n",
    "    StructField(\"Dim3\", StringType(), True),\n",
    "\n",
    "    StructField(\"DataSourceDimType\", StringType(), True),\n",
    "    StructField(\"DataSourceDim\", StringType(), True),\n",
    "\n",
    "    StructField(\"Value\", StringType(), True),\n",
    "    StructField(\"NumericValue\", DoubleType(), True),\n",
    "\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"TimeDimensionValue\", StringType(), True),\n",
    "    StructField(\"TimeDimensionBegin\", StringType(), True),\n",
    "    StructField(\"TimeDimensionEnd\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Criamos um RDD (Resilient Distributed Dataset) a partir do JSON e depois um DataFrame.\n",
    "# Isso permite que o Spark distribua o processamento dos dados.\n",
    "def criar_df_tratado(dados):\n",
    "    json_strings = [json.dumps(item) for item in dados]\n",
    "    rdd = spark.sparkContext.parallelize(json_strings)\n",
    "    df = spark.read.json(rdd, schema=schema)\n",
    "    \n",
    "    # Aplicando transformações para limpar e organizar os dados\n",
    "    df_tratado = df.select(\n",
    "        col(\"SpatialDim\").alias(\"Pais\"),\n",
    "        col(\"TimeDim\").alias(\"Ano\"),\n",
    "        col(\"Dim1\").alias(\"Sexo\"),\n",
    "        col(\"NumericValue\").alias(\"Valor\")\n",
    "    )\n",
    "    \n",
    "    return df_tratado\n",
    "\n",
    "df_suicidio_b = criar_df_tratado(dados_suicidio_b)\n",
    "df_suicidio_f = criar_df_tratado(dados_suicidio_f)\n",
    "df_suicidio_m = criar_df_tratado(dados_suicidio_m)\n",
    "\n",
    "df_depressao_b = criar_df_tratado(dados_depressao_b)\n",
    "df_depressao_f = criar_df_tratado(dados_depressao_f)\n",
    "df_depressao_m = criar_df_tratado(dados_depressao_m)\n",
    "\n",
    "print(\"Mostrando Tabelas de Suicidio de cada Sexo:\")\n",
    "#Ambos\n",
    "df_suicidio_b.printSchema()\n",
    "df_suicidio_b.show(3, False)\n",
    "\n",
    "#Mulher\n",
    "df_suicidio_f.printSchema()\n",
    "df_suicidio_f.show(3, False)\n",
    "\n",
    "#Homem\n",
    "df_suicidio_m.printSchema()\n",
    "df_suicidio_m.show(3, False)\n",
    "\n",
    "print(\"Mostrando Tabelas de Depressao de cada Sexo:\")\n",
    "#Ambos\n",
    "df_depressao_b.printSchema()\n",
    "df_depressao_b.show(3, False)\n",
    "\n",
    "#Mulher\n",
    "df_depressao_f.printSchema()\n",
    "df_depressao_f.show(3, False)\n",
    "\n",
    "#Homem\n",
    "df_depressao_m.printSchema()\n",
    "df_depressao_m.show(3, False)\n",
    "\n",
    "# --- 4. Carga (Load) ---\n",
    "# Salvamos o DataFrame tratado no HDFS em formato Parquet.\n",
    "# Parquet é um formato colunar otimizado para performance em análises com Spark.\n",
    "\n",
    "# O caminho no HDFS onde os dados serão salvos.\n",
    "HDFS_SILVER_SB = \"hdfs://namenode:9000/datalake/silver/suicidio_b\"\n",
    "HDFS_SILVER_SF = \"hdfs://namenode:9000/datalake/silver/suicidio_f\"\n",
    "HDFS_SILVER_SM = \"hdfs://namenode:9000/datalake/silver/suicidio_m\"\n",
    "\n",
    "HDFS_SILVER_DB = \"hdfs://namenode:9000/datalake/silver/depressao_b\"\n",
    "HDFS_SILVER_DF = \"hdfs://namenode:9000/datalake/silver/depressao_f\"\n",
    "HDFS_SILVER_DM = \"hdfs://namenode:9000/datalake/silver/depressao_m\"\n",
    "\n",
    "print(f\"Salvando dados tratados no HDFS...\")\n",
    "\n",
    "# 'overwrite' substitui os dados se o diretório já existir.\n",
    "# 'partitionBy' é útil para organizar os dados, mas como já filtramos por ano_mes,\n",
    "# incluímos ele no caminho para criar uma partição manual.\n",
    "df_suicidio_b.write.mode(\"overwrite\").parquet(HDFS_SILVER_SB)\n",
    "df_suicidio_f.write.mode(\"overwrite\").parquet(HDFS_SILVER_SF)\n",
    "df_suicidio_m.write.mode(\"overwrite\").parquet(HDFS_SILVER_SM)\n",
    "\n",
    "df_depressao_b.write.mode(\"overwrite\").parquet(HDFS_SILVER_DB)\n",
    "df_depressao_f.write.mode(\"overwrite\").parquet(HDFS_SILVER_DF)\n",
    "df_depressao_m.write.mode(\"overwrite\").parquet(HDFS_SILVER_DM)\n",
    "\n",
    "print(\"Dados salvos com sucesso no HDFS!\")\n",
    "\n",
    "def renomear_coluna(df, coluna, nome_novo):\n",
    "    return df.withColumnRenamed(coluna, nome_novo)\n",
    "\n",
    "print(\"Renomeando coluna Valor de acordo com a tabela:\")\n",
    "\n",
    "df_suicidio_b = renomear_coluna(df_suicidio_b, \"Valor\", \"Suicidio_Ambos\")\n",
    "df_suicidio_f = renomear_coluna(df_suicidio_f, \"Valor\", \"Suicidio_Mulher\")\n",
    "df_suicidio_m = renomear_coluna(df_suicidio_m, \"Valor\", \"Suicidio_Homem\")\n",
    "\n",
    "df_depressao_b = renomear_coluna(df_depressao_b, \"Valor\", \"Depressao_Ambos\")\n",
    "df_depressao_f = renomear_coluna(df_depressao_f, \"Valor\", \"Depressao_Mulher\")\n",
    "df_depressao_m = renomear_coluna(df_depressao_m, \"Valor\", \"Depressao_Homem\")\n",
    "\n",
    "print(\"Colunas renomeadas!\")\n",
    "\n",
    "print(\"DataFrames Silver carregados:\")\n",
    "\n",
    "print(\"Mostrando Tabelas de Suicidio de cada Sexo:\")\n",
    "df_suicidio_b.show(3)\n",
    "df_suicidio_f.show(3)\n",
    "df_suicidio_m.show(3)\n",
    "\n",
    "print(\"Mostrando Tabelas de Depressao de cada Sexo:\")\n",
    "df_depressao_b.show(3)\n",
    "df_depressao_f.show(3)\n",
    "df_depressao_m.show(3)\n",
    "\n",
    "# ======================\n",
    "# 5. JOIN (Camada GOLD)\n",
    "# ======================\n",
    "df_gold = (df_suicidio_b\n",
    "           .drop(\"Sexo\")\n",
    "           .join(df_depressao_b.drop(\"Sexo\"), [\"Pais\", \"Ano\"], \"outer\")\n",
    "           .join(df_suicidio_f.drop(\"Sexo\"), [\"Pais\", \"Ano\"], \"outer\")\n",
    "           .join(df_depressao_f.drop(\"Sexo\"), [\"Pais\", \"Ano\"], \"outer\")\n",
    "           .join(df_suicidio_m.drop(\"Sexo\"), [\"Pais\", \"Ano\"], \"outer\")\n",
    "           .join(df_depressao_m.drop(\"Sexo\"), [\"Pais\", \"Ano\"], \"outer\")\n",
    "    )\n",
    "\n",
    "print(\"Resultado do join GOLD:\")\n",
    "df_gold.show(10, truncate=False)\n",
    "\n",
    "# ======================\n",
    "# 6. Caminho GOLD\n",
    "# ======================\n",
    "\n",
    "CAMINHO_GOLD = \"hdfs://namenode:9000/datalake/gold/suicidio_depressao\"\n",
    "\n",
    "# ======================\n",
    "# 7. Salvando em PARQUET\n",
    "# ======================\n",
    "\n",
    "df_gold.write.mode(\"overwrite\").parquet(CAMINHO_GOLD)\n",
    "\n",
    "print(\"Arquivo GOLD salvo em Parquet com sucesso!\")\n",
    "\n",
    "# ======================\n",
    "# 8. (Opcional) Salvar também em CSV – ideal para Power BI\n",
    "# ======================\n",
    "url = \"jdbc:postgresql://postgres:5432/superset\"\n",
    "tabela = \"fato_suicidio_depressao\"\n",
    "props = {\n",
    "    \"user\": \"superset\",\n",
    "    \"password\": \"superset\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df_gold.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .jdbc(url=url, table=tabela, properties=props)\n",
    "\n",
    "print(\"Tabela salva com sucesso no PostgreSQL!\")\n",
    "\n",
    "# --- 5. Finalização ---\n",
    "spark.stop()\n",
    "print(\"Sessão Spark finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee5e56-ff5a-4e8d-bf11-7d3f27954567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
